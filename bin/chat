#!python
from datetime import datetime
from openai import OpenAI
import argparse
import sys
import signal
import os
import tempfile
from typing import Any, Literal
import openai
import json
import subprocess
import re
from pydub import AudioSegment
from pydub.playback import play
import io

TTS_VOICES = Literal["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
response_history_file = response_history_file = os.path.expanduser(
    "~/scripts/chat_response_history.json"
)


def save_response(response: Any):
    """Saves the response to a file."""
    with open(response_history_file, "a") as f:
        f.write(json.dumps(response, indent=2) + "\n")


def extract_code_blocks(text: str):
    """Extracts all lines between triple backticks (` ``` `) from a given text."""
    results = re.findall(r"```(.*?)```", text, re.DOTALL)
    by_language: dict[str, list[Any]] = {}
    for result in results:
        if result.startswith("\n"):
            key = "unknown"
            code = result
        else:
            key, *code = result.split("\n", 1)
        if key not in by_language:
            by_language[key] = []
        by_language[key].append(code)
    return by_language


def open_in_vim(file_path: str, start_byte_offset: int, response_byte_offset: int):
    """Open all the saved temporary files in a new Vim session."""
    subprocess.run(
        [
            "vim",
            file_path,
            f"+goto{start_byte_offset}",
            "+:normal zfgg",
            f"+goto{response_byte_offset}",
        ]
    )


class ChatFile:
    def __init__(self):
        self.chat_file = open(self.get_chat_file(), "a")

    def write(self, prompt: str, response: str):
        start_pos = self.chat_file.tell()
        start_section = "".join(
            [
                "# Asked at: ",
                " ",
                datetime.now().strftime("%Y-%m-%dT%H-%M-%S"),
                "\n\n",
            ]
        )
        sep = "\n" + "-" * 64 + "\n"
        header = start_section + prompt + sep
        self.chat_file.write(header)
        self.chat_file.write(response + "\n")

        return start_pos, start_pos + len(header.encode("utf-8")) + 1

    def get_chat_file(self):
        """Returns the path to the chat file."""
        return os.path.expanduser("~/scripts/chats/chat-1.md")


def main(prompt: str, open_response: bool = False, code: bool = False):
    # Load your API key from an environment variable or secret management service
    openai.api_key = os.getenv("OPENAI_API_KEY")

    model = (
        "gpt-4-1106-preview"
        if not (prompt.strip().startswith("fast") or prompt.strip().endswith("fast"))
        else "gpt-3.5-turbo"
    )

    def print_response(response, res_data: list[str]):
        if len(response.choices) > 1:
            raise ValueError("Expected one response")
        delta = response.choices[0].delta
        if not delta or delta.content is None:
            print("\n\n")
            res_data.append("\n\n")
            return ""
        x = delta.content
        res_data.append(x)
        print(x, end="")
        return delta.content

    # ChatCompletionUserMessageParam
    print(f"* {prompt} *")
    chat_completion = openai.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "You provide responses in markdown format, but never use top level # markdown sections.",
            },
            {
                "role": "system",
                "content": "For answers that contain code, you always provide code blocks as the very first thing in your response.",
            },
            {
                "role": "system",
                "content": "If a question begins with the word 'short', limit your response to 10 lines or less and favor examples over explanations.",
            },
            {"role": "user", "content": prompt},
        ],
        stream=True,
    )

    response_cache = []
    res_text: list[str] = []

    def vim_thunk():
        start_position, response_position = ChatFile().write(prompt, "".join(res_text))
        open_in_vim(ChatFile().get_chat_file(), start_position, response_position)

    def signal_handler(sig, frame):
        vim_thunk()
        sys.exit(1)

    signal.signal(signal.SIGINT, signal_handler)
    for i, chunk in enumerate(chat_completion):
        if i == 0:
            print(f"-----------{model}--------------------\n")
            base_message = chunk

        content = print_response(chunk, res_text)
        if content:
            response_cache.append(content)
    vim_thunk()


def stream_and_play(text: str, voice: TTS_VOICES, save: str | None = None):
    client = OpenAI()
    if save:
        save_path = os.path.abspath(os.path.expanduser(save))

        if not os.path.exists(os.path.dirname(save_path)):
            raise ValueError(f"Directory {os.path.dirname(save_path)} does not exist")
        save_response(text)
        response = client.audio.speech.create(
            model="tts-1-hd",
            voice=voice,
            speed=1.2,
            input=text,
        )
        print(f"Saving to {save_path}")
        response.stream_to_file(save_path)
    else:
        response = client.audio.speech.create(
            model="tts-1",
            voice="alloy",
            input=text,
        )

        # Convert the binary response content to a byte stream
        byte_stream = io.BytesIO(response.content)

        # Read the audio data from the byte stream
        audio = AudioSegment.from_file(byte_stream, format="mp3")

        # Play the audio
        play(audio)


def interactive_get_file(starting_text: str = ""):
    """Returns the path to the chat file."""
    temp_file = tempfile.NamedTemporaryFile(suffix=".md")
    temp_file.write(starting_text.encode("utf-8"))
    temp_file.flush()
    # use supproccess to open vim
    res = subprocess.run(["vim", temp_file.name])
    if res.returncode != 0:
        sys.exit(1)
    return open(temp_file.name).read()


if __name__ == "__main__":
    main_parser = argparse.ArgumentParser(description="OpenAI GPT-4 API interaction.")

    interactive_parser = argparse.ArgumentParser(
        description="OpenAI GPT-4 API interaction."
    )
    main_parser.add_argument(
        "prompt", nargs="+", help="The prompt string to send to GPT-4"
    )
    for parser in [main_parser, interactive_parser]:
        parser.add_argument(
            "--interactive",
            "-i",
            action="store_true",
            default=False,
        )
        parser.add_argument(
            "--speak",
            action="store_true",
            default=False,
        )
        parser.add_argument(
            "--voice",
            choices=["alloy", "echo", "fable", "onyx", "nova", "shimmer"],
            default="onyx",
            help="Speak the response",
        )
        parser.add_argument(
            "--save",
            default=None,
            help="Speak the response",
        )

    if "--interactive" in sys.argv or "-i" in sys.argv:
        sys.argv.append(" ")
    args = main_parser.parse_args(args=sys.argv[1:])
    p = " ".join(args.prompt).strip()
    if p == "-":
        p = sys.stdin.read()
    if args.interactive:
        prompt = interactive_get_file(p)
    else:
        prompt = p

    if not prompt:
        sys.exit(1)

    if args.speak:
        if prompt == "-":
            stream_and_play(sys.stdin.read(), voice=args.voice, save=args.save)
        else:
            stream_and_play(prompt, voice=args.voice, save=args.save)
    else:
        main(prompt, open_response=True)
